
# Configuration et Installation de l'Environnement pour un Projet RAG en Python

Pour travailler sur un projet Python tel qu'un syst√®me de RAG (Retrieval-Augmented Generation) pour l'assistance juridique, il est essentiel d'avoir un environnement bien configur√©. Ce guide vous aidera √† installer et configurer l'environnement de d√©veloppement en utilisant Python et/ou Anaconda.



## √âtape 0 : V√©rifier et Installer Python
Avant de commencer, assurez-vous d'avoir un interpr√©teur Python install√© sur votre machine. Il existe deux principales options :

- **Installer Anaconda** : Recommand√© si vous travaillez avec du data science ou du machine learning. [T√©l√©charger Anaconda](https://www.anaconda.com/download)
- **Installer Python directement** : Option plus l√©g√®re et flexible. [T√©l√©charger Python.](https://www.python.org/downloads/)

V√©rifiez si Python/anconda est d√©j√† install√© en ex√©cutant la commande suivante dans un terminal :

```shell
python --version
```

Si Python est install√©, vous verrez une sortie indiquant la version, par exemple : \texttt{Python 3.11.2}.
Si ce n'est pas le cas, installez Python en suivant l'un des liens ci-dessus.



## √âtape 1 : Cr√©ation d'un Environnement Virtuel
Il est recommand√© de cr√©er un environnement virtuel pour chaque projet afin d'√©viter les conflits entre diff√©rentes versions de biblioth√®ques.

#### Cr√©ation d'un Environnement avec Conda
Si vous utilisez Anaconda, cr√©ez un environnement d√©di√© avec la commande suivante :
```shell
conda create -n mon_projet python=3.11
```

#### Activation de l'Environnement Conda
```shell
conda activate mon_projet
```

#### Cr√©ation d'un Environnement Virtuel avec venv (sans Anaconda)
Si vous pr√©f√©rez utiliser Python sans Anaconda, utilisez **venv** pour cr√©er un environnement virtuel :
```shell
python -m venv mon_projet_env
```

#### Activation de l'Environnement Python (venv)

**Sur Windows** :
```shell
mon_projet_env\Scripts\activate
```
**Sur macOS/Linux** :

```shell
source mon_projet_env/bin/activate
```


Apr√®s activation, vous verrez probablement le nom de votre environnement affich√© entre parenth√®ses dans votre terminal.



## √âtape 2 : Installation des Biblioth√®ques N√©cessaires
Une fois l'environnement activ√©, installez les biblioth√®ques requises pour votre projet. Ces d√©pendances sont g√©n√©ralement list√©es dans un fichier **requirements.txt**.

#### Installation des Packages avec pip
Si vous avez un fichier \texttt{requirements.txt}, installez toutes les d√©pendances en une seule commande :
```shell
pip install -r requirements.txt
```

#### V√©rification des Installations
Apr√®s l'installation, v√©rifiez que les biblioth√®ques ont bien √©t√© install√©es en listant les packages disponibles :
```shell
pip list
```



## √âtape 3 : V√©rifier et Tester l'Environnement
Pour s'assurer que tout fonctionne correctement, vous pouvez ex√©cuter une simple commande Python dans votre environnement activ√© :
```Python
import sys
print(sys.version)

```

Si la version de Python affich√©e correspond √† celle de votre environnement, alors tout est bien configur√©.

Vous avez maintenant un environnement Python propre et bien configur√© pour travailler sur votre projet RAG pour l'assistance juridique. Il ne vous reste plus qu'√† commencer √† d√©velopper votre application !



## √âtape 4 : Cr√©ation du Projet
Nous allons maintenant cr√©er un dossier pour notre projet sous le nom legal_rag.

1. Ajouter une Variable d'Environnement
Pour ce projet, nous avons besoin d'utiliser une cl√© API provenant d'OpenAI. Pour cela, nous allons cr√©er un fichier .env et y ajouter notre cl√© API.

2. Ajouter un Dossier de Donn√©es
Nous allons √©galement ajouter un dossier data o√π nous stockerons les donn√©es n√©cessaires au projet.




# RAG et base de donner vecotriel


## Indexation des Documents

L'indexation est une √©tape essentielle dans la construction d'un syst√®me RAG (Retrieval-Augmented Generation).
Elle consiste √† transformer les documents sources en embeddings afin qu'ils puissent √™tre stock√©s et recherch√©s efficacement dans une base de donn√©es vectorielle.
### Chargement des Documents PDF

Nous allons commencer par t√©l√©verser un dossier contenant des fichiers PDF et les manipuler avec la biblioth√®que PyPDF de LangChain.
Exemple d'utilisation de PyPDF pour extraire du texte

```Python

from langchain.document_loaders import PyPDFLoader

# Charger un document PDF
loader = PyPDFLoader("chemin/vers/le/document.pdf")
documents = loader.load()

# Afficher le contenu extrait
documents[:2]  # Affiche les deux premiers segments extraits

```
### Segmentation des Documents en Chunks
Une fois le texte extrait, nous devons le diviser en morceaux (chunks) pour faciliter l'indexation et la recherche.


M√©thodes de Chunking dans LangChain:

- RecursiveCharacterTextSplitter : Divise le texte en utilisant des caract√®res sp√©cifiques comme limite.

- CharacterTextSplitter : Utilise un caract√®re (ex. '\n' ou ' ') pour scinder le texte.

- TokenTextSplitter : Divise le texte en fonction du nombre de tokens.

- NLTKTextSplitter : Utilise la biblioth√®que NLTK pour le traitement du langage naturel.

- SentenceTransformersTextSplitter : Utilise des mod√®les de transformer pour diviser intelligemment le texte.


Exemple d'utilisation de RecursiveCharacterTextSplitter
```Python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # Nombre de caract√®res par chunk
    chunk_overlap=50  # Chevauchement entre les chunks
)

chunks = text_splitter.split_documents(documents)
print(f"Nombre de chunks g√©n√©r√©s: {len(chunks)}")

```

Stockage des Chunks dans une Base de Donn√©es Vectorielle (ChromaDB)

Nous allons stocker ces chunks sous forme de vecteurs dans ChromaDB.
```Python
import chromadb
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# Initialiser ChromaDB
chroma_client = chromadb.PersistentClient(path="./chroma_db")

# Convertir les chunks en embeddings et les stocker
embedding_function = OpenAIEmbeddings()
vector_store = Chroma.from_documents(chunks, embedding_function, client=chroma_client)
```

## R√©cup√©ration des Informations (Retrieval)
Une fois les embeddings stock√©s, nous pouvons cr√©er un objet retriever pour rechercher les documents les plus pertinents.

Cr√©ation d'un Retriever avec ChromaDB

```Python
    retriever = vector_store.as_retriever()
```

Exemples d'Utilisation de la R√©cup√©ration d'Informations

```Python
query = "Quels sont les r√®glements juridiques pour les contrats de travail ?"
retrieved_docs = retriever.get_relevant_documents(query)

# Afficher les r√©sultats
for doc in retrieved_docs:
    print(doc.page_content)
```

## G√©n√©ration de R√©ponses Bas√©es sur les Documents R√©cup√©r√©s
Apr√®s avoir r√©cup√©r√© les documents pertinents, nous pouvons les utiliser pour enrichir une r√©ponse g√©n√©r√©e par un mod√®le de langage.

Example:
```Python
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

llm = OpenAI(model_name="gpt-4", temperature=0.5)
qa_chain = RetrievalQA(llm=llm, retriever=retriever)

query = "Quels sont les droits d'un employ√© en cas de licenciement ?"
response = qa_chain.run(query)
print(response)
```

## Limitations de cette Approche Na√Øve et Am√©liorations

Bien que cette approche de RAG fonctionne, elle pr√©sente plusieurs limitations :

Recherches bas√©es uniquement sur la similarit√© vectorielle : Le mod√®le peut retourner des r√©sultats pertinents en termes de distance dans l'espace des embeddings, mais pas forc√©ment en fonction de la pertinence s√©mantique exacte.

Manque de pond√©ration des r√©sultats : Certains documents plus pertinents peuvent √™tre noy√©s dans la masse.

Absence de fusion des r√©sultats multi-sources : Lorsqu'un m√™me sujet est trait√© dans plusieurs documents, l'agr√©gation d'information est limit√©e.

Manque de post-traitement intelligent : Les r√©sultats bruts sont retourn√©s sans r√©organisation ou hi√©rarchisation avanc√©e.

Am√©liorations avec des Techniques Avanc√©es de RAG

Pour surmonter ces limitations, on peut utiliser des techniques avanc√©es comme Reciprocal Rank Fusion (RRF) et d'autres m√©thodes de re-ranking :

1. Reciprocal Rank Fusion (RRF) :

Fusionne les r√©sultats de plusieurs requ√™tes ou m√©thodes de recherche.

Am√©liore la pertinence en combinant plusieurs scores de recherche.

Extrait les meilleurs r√©sultats en donnant une pond√©ration plus √©quilibr√©e.

Impl√©mentation de RRF avec ChromaDB

```Python
def reciprocal_rank_fusion(results_list, k=60):
    fused_scores = {}
    for rank, doc in enumerate(results_list, start=1):
        fused_scores[doc] = fused_scores.get(doc, 0) + 1 / (k + rank)
    return sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
```

2. Re-ranking avec un mod√®le Transformer :

Utiliser un mod√®le comme ColBERT pour am√©liorer la qualit√© des r√©sultats r√©cup√©r√©s.

Appliquer un reranking bas√© sur l'attention multi-head.

3. Fusion Multi-Sources :

Enrichir les r√©sultats en combinant plusieurs sources de donn√©es externes.

Appliquer une pond√©ration dynamique en fonction du contexte de la requ√™te.

Avec ces techniques, on peut significativement am√©liorer la pertinence des r√©sultats retourn√©s et optimiser la performance du syst√®me RAG pour une meilleure assistance √† la prise de d√©cision


# Getting Started with Streamlit
## Creating Your First Streamlit App
A simple Streamlit app is created using a Python script. Example:

```Python

import streamlit as st

st.title("Hello, Streamlit! üëã")
st.write("This is my first Streamlit app.")

```


Save this as app.py and run it using:

```sh
streamlit run app.py
```

This will launch a local web server, and the app will open in a browser.


## Adding Widgets
Widgets allow user interaction:

```Python
name = st.text_input("Enter your name:")
st.write(f"Hello, {name}!")

```

Other widgets include:
- st.button("Click me")
- st.slider("Select a value", 0, 100)
- st.checkbox("Agree to terms")

## Displaying Data

Tables and DataFrames

```Python
import pandas as pd

df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35]
})

st.table(df)
st.dataframe(df)  # Allows scrolling

```


Charts and Plots
```Python
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(0, 10, 100)
y = np.sin(x)

fig, ax = plt.subplots()
ax.plot(x, y)
st.pyplot(fig)

```

## Handling User Input

```Python
if st.button("Say Hello"):
    st.write("Hello, World!")
```

For file uploads:
```Python

uploaded_file = st.file_uploader("Choose a file")
if uploaded_file is not None:
    st.write("File uploaded successfully!")

```

## Layout and Organization

clumns
```Python

col1, col2 = st.columns(2)
col1.write("Column 1")
col2.write("Column 2")

```

Sidebar
```python

st.sidebar.title("Sidebar Menu")
st.sidebar.button("Click me")

```

Tabs

```

import streamlit as st

st.title("Streamlit Tabs Example")

tab1, tab2, tab3 = st.tabs(["Home", "About", "Contact"])

with tab1:
    st.header("üè† Welcome to Home")
    st.write("This is the main dashboard.")

with tab2:
    st.header("üìñ About")
    st.write("This tab contains information about the project.")

with tab3:
    st.header("üì© Contact")
    st.write("You can reach us at example@example.com.")

```



## chat with llm

```Python
import streamlit as st
import openai

# Set up the OpenAI API key (use environment variables in production)
openai.api_key = "your-api-key"

st.title("üí¨ Chat with LLM")

# Create chat history using session state
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display previous chat messages
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Input field for user messages
user_input = st.chat_input("Type your message...")
if user_input:
    # Add user message to session state
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    with st.chat_message("user"):
        st.markdown(user_input)

    # Generate response from OpenAI
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": msg["role"], "content": msg["content"]} for msg in st.session_state.messages]
    )

    bot_response = response["choices"][0]["message"]["content"]

    # Add bot message to session state
    st.session_state.messages.append({"role": "assistant", "content": bot_response})

    with st.chat_message("assistant"):
        st.markdown(bot_response)

```


# Final Project

create legal rag adive using rag and streamlit